{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ_QwlqxXkeZ",
        "outputId": "bd9b9cc2-1bd0-4542-bfa7-3f4474cb026a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Total Error: [3]\n",
            "Epoch 2/20 - Total Error: [3]\n",
            "Epoch 3/20 - Total Error: [3]\n",
            "Epoch 4/20 - Total Error: [1]\n",
            "Epoch 5/20 - Total Error: [1]\n",
            "Epoch 6/20 - Total Error: [2]\n",
            "Epoch 7/20 - Total Error: [1]\n",
            "Epoch 8/20 - Total Error: [0]\n",
            "Epoch 9/20 - Total Error: [0]\n",
            "Epoch 10/20 - Total Error: [0]\n",
            "Epoch 11/20 - Total Error: [0]\n",
            "Epoch 12/20 - Total Error: [0]\n",
            "Epoch 13/20 - Total Error: [0]\n",
            "Epoch 14/20 - Total Error: [0]\n",
            "Epoch 15/20 - Total Error: [0]\n",
            "Epoch 16/20 - Total Error: [0]\n",
            "Epoch 17/20 - Total Error: [0]\n",
            "Epoch 18/20 - Total Error: [0]\n",
            "Epoch 19/20 - Total Error: [0]\n",
            "Epoch 20/20 - Total Error: [0]\n",
            "\n",
            "Final Weights: [0.17454012 0.35071431]\n",
            "Final Bias: [-0.46800606]\n",
            "Input: [0 0] => Output: [0]\n",
            "Input: [0 1] => Output: [0]\n",
            "Input: [1 0] => Output: [0]\n",
            "Input: [1 1] => Output: [1]\n",
            "\n",
            "Accuracy: 4 / 4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creating a dataset\n",
        "# Inputs [x1, x2]\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Labels for AND gate\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initializing parameters\n",
        "np.random.seed(42)  # for reproducibility\n",
        "weights = np.random.rand(2)  # one weight per input\n",
        "bias = np.random.rand(1)     # bias term\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Activation function\n",
        "def step_function(z):\n",
        "    return int(z >= 0) if np.isscalar(z) or z.shape == () else np.where(z >= 0, 1, 0)\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "    for xi, target in zip(X, y):\n",
        "        # Forward pass\n",
        "        z = np.dot(xi, weights) + bias\n",
        "        output = step_function(z)\n",
        "\n",
        "        # Error\n",
        "        error = target - output\n",
        "        total_error += abs(error)\n",
        "\n",
        "        # Perceptron Learning Rule (Updating weights)\n",
        "        weights += learning_rate * error * xi\n",
        "        bias += learning_rate * error\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Total Error: {total_error}\")\n",
        "\n",
        "# Testing\n",
        "print(\"\\nFinal Weights:\", weights)\n",
        "print(\"Final Bias:\", bias)\n",
        "\n",
        "for xi in X:\n",
        "    z = np.dot(xi, weights) + bias\n",
        "    output = step_function(z)\n",
        "    print(f\"Input: {xi} => Output: {output}\")\n",
        "\n",
        "print(\"\\nAccuracy:\", sum((step_function(np.dot(x, weights) + bias) == y_i).item() for x, y_i in zip(X, y)), \"/\", len(y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pennylane\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "\n",
        "# Inputs [x1, x2]\n",
        "X = pnp.array([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "\n",
        "y = pnp.array([0., 0., 0., 1.])\n",
        "\n",
        "# Quantum device\n",
        "dev = qml.device(\"default.qubit\", wires=1, shots=None)\n",
        "\n",
        "# Single Qubit Model\n",
        "@qml.qnode(dev)\n",
        "def circuit(x, w):\n",
        "    # Feature encoding (round 1)\n",
        "    qml.RX(pnp.pi/2 * x[0], wires=0)\n",
        "    qml.RY(pnp.pi/2 * x[1], wires=0)\n",
        "    # Interaction term (x1 * x2) to help separate [1,0] vs [1,1]\n",
        "    qml.RZ(pnp.pi/2 * (x[0]*x[1]), wires=0)\n",
        "\n",
        "    # Trainable block 1\n",
        "    qml.Rot(w[0], w[1], w[2], wires=0)\n",
        "\n",
        "    # Feature encoding (round 2)\n",
        "    qml.RX(pnp.pi/2 * x[0], wires=0)\n",
        "    qml.RY(pnp.pi/2 * x[1], wires=0)\n",
        "\n",
        "    # Trainable block 2\n",
        "    qml.Rot(w[3], w[4], w[5], wires=0)\n",
        "\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "def model(x, w):\n",
        "    # Map ⟨Z⟩ ∈ [-1,1] to probability ∈ [0,1]\n",
        "    expval = circuit(x, w)\n",
        "    return (1 - expval) / 2.0\n",
        "\n",
        "# Binary Cross Entropy Loss\n",
        "def loss(w):\n",
        "    eps = 1e-7\n",
        "    preds = pnp.array([model(xi, w) for xi in X])\n",
        "    preds = pnp.clip(preds, eps, 1 - eps)\n",
        "    return -pnp.mean(y * pnp.log(preds) + (1 - y) * pnp.log(1 - preds))\n",
        "\n",
        "# Train\n",
        "pnp.random.seed(7)\n",
        "w = pnp.random.uniform(0, 2*pnp.pi, 6, requires_grad=True)\n",
        "opt = qml.AdamOptimizer(stepsize=0.1)\n",
        "epochs = 200\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    w, c = opt.step_and_cost(loss, w)\n",
        "    if epoch % 20 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:3d} - loss: {c:.6f}\")\n",
        "\n",
        "# Evaluation\n",
        "def predict(x, w, threshold=0.5):\n",
        "    p = float(model(x, w))\n",
        "    return (1 if p >= threshold else 0), p\n",
        "\n",
        "print(\"\\nLearned parameters:\", [float(a) for a in w])\n",
        "\n",
        "correct = 0\n",
        "for xi, yi in zip(X, y):\n",
        "    cls, prob = predict(xi, w)\n",
        "    correct += (cls == int(yi))\n",
        "    print(f\"Input {xi.astype(int)} -> prob(1)={prob:.3f} -> pred={cls} (target={int(yi)})\")\n",
        "\n",
        "print(f\"\\nAccuracy: {correct}/{len(X)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAUCxbWhYZ3F",
        "outputId": "2773802b-ade9-4a9b-d82a-76ee67740441"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 - loss: 0.525762\n",
            "Epoch  20 - loss: 0.209820\n",
            "Epoch  40 - loss: 0.176702\n",
            "Epoch  60 - loss: 0.174517\n",
            "Epoch  80 - loss: 0.174259\n",
            "Epoch 100 - loss: 0.174175\n",
            "Epoch 120 - loss: 0.174174\n",
            "Epoch 140 - loss: 0.174174\n",
            "Epoch 160 - loss: 0.174173\n",
            "Epoch 180 - loss: 0.174173\n",
            "Epoch 200 - loss: 0.174173\n",
            "\n",
            "Learned parameters: [-0.9113396581032276, 4.479204828296538, 4.052927885892655, 4.7123834823991375, 4.712365871730409, 3.3834693422771163]\n",
            "Input [0 0] -> prob(1)=0.116 -> pred=0 (target=0)\n",
            "Input [0 1] -> prob(1)=0.202 -> pred=0 (target=0)\n",
            "Input [1 0] -> prob(1)=0.116 -> pred=0 (target=0)\n",
            "Input [1 1] -> prob(1)=0.798 -> pred=1 (target=1)\n",
            "\n",
            "Accuracy: 4/4\n"
          ]
        }
      ]
    }
  ]
}